<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="https://maxweber.github.io/" rel="self" type="application/rss+xml"/><title>The blog of Max Weber</title><link>https://maxweber.github.io/</link><description>A blog about software and business development</description><lastBuildDate>Wed, 19 Jun 2019 13:15:18 +0200</lastBuildDate><generator>clj-rss</generator><item><guid>https://maxweber.github.io/blog/2019-06-15-approaching-the-web-after-tomorrow-part-3</guid><link>https://maxweber.github.io/blog/2019-06-15-approaching-the-web-after-tomorrow-part-3</link><title>Approaching The Web After Tomorrow - Part 3</title><description>&lt;p&gt;This blog post series is about my attempt to implement &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;The Web After Tomorrow&lt;/a&gt; that &lt;a href='https://twitter.com/nikitonsky'&gt;Nikita&lt;/a&gt; described in his &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;blog post&lt;/a&gt; back in 2015.&lt;/p&gt;&lt;p&gt;In &lt;a href='https://maxweber.github.io/blog/2019-06-04-approaching-the-web-after-tomorrow'&gt;part 1&lt;/a&gt; I talked about how we used DataScript, Datomic and Clojure(Script) to build a real-time app for our SaaS product &lt;a href='https://www.storrito.com'&gt;Storrito.com&lt;/a&gt; and what performance challenge the first implementation yielded. &lt;a href='https://maxweber.github.io/blog/2019-06-08-approaching-the-web-after-tomorrow-part-2'&gt;Part 2&lt;/a&gt; was about some ideas I had to solve this performance challenge.&lt;/p&gt;&lt;p&gt;This part will be about reconsidering some common trade-offs of web application development to bring up a solution that has the potential to save a lot of effort and accidential complexity, especially for small teams of developers.&lt;/p&gt;&lt;p&gt;I must admit that I had no good idea for a couple of months how to solve the described performance issue, without generating tons of effort for our team. Oftentimes it is valuable to make a pause, work on other topics and &lt;a href='https://www.youtube.com/watch?v=f84n5oFoZBc'&gt;let your brain do some background processing&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;One morning I woke up with the question, why it still so much &lt;a href='https://www.infoq.com/presentations/Simple-Made-Easy/'&gt;simpler&lt;/a&gt; to build a classic server-side web application in comparison to a single-page web application (SPA). Virtual DOM frameworks like React already made a step in the right direction by providing a very simple mental model (at its core) to create user interfaces. In essence they allow you to build your UI like a classic website: you start with a blank screen and the browser renders the complete HTML. You do not have to think about what is currently shown on the screen and how to modify the DOM to the desired new UI state.&lt;/p&gt;&lt;p&gt;A classic server-side web application often does one or more database queries to generate the HTML string that is sent to the browser (let's say to render a HTML table of customer addresses for example). Maybe there is another simple mental model waiting for us, if we design the API for our SPA more like how things are done within a classic server-side web application.&lt;/p&gt;&lt;p&gt;Instead of serving the complete HTML for a webpage, we could just return the required data from the database query results via the API response and let the client do the HTML rendering.&lt;/p&gt;&lt;p&gt;Nothing new you might think in comparison to a REST-based API, but the key difference is that this API response is only meant for this single webpage (like the customers table UI). REST APIs often have endpoints like '/customers', where you get a JSON response with customer entries from the database. But there the accidential complexity starts to emerge:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;what parts of your client application use this endpoint.&lt;/li&gt;&lt;li&gt;what fields of the customer table do they need (first-name,  last-name, birthday etc.).&lt;/li&gt;&lt;li&gt;one UI element also needs the date of the customer's last order, which is  not part of the '/customers' API endpoint (n+1 problem).&lt;/li&gt;&lt;li&gt;is the logged-in user authorized to see these customer entries&lt;/li&gt;&lt;li&gt;which developer teams in your organization are using this  '/customers' API endpoint.&lt;/li&gt;&lt;li&gt;where can they find your API documentation.&lt;/li&gt;&lt;li&gt;if it is part of a public API, even more questions arise.&lt;/li&gt;&lt;li&gt;often a different authentication is required (like OAuth2)&lt;/li&gt;&lt;li&gt;is the change backwards-compatible or is it a breaking change  (&lt;a href='https://www.youtube.com/watch?v=oyLBGkS5ICk'&gt;btw don't do this&lt;/a&gt;) and  you need to version your API endpoint (like '/v2/customers').&lt;/li&gt;&lt;li&gt;has the quota of the external API caller been reached.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I could continue with more points for a while, but I guess you get the point, you need to take care of dozens of other concerns that have nothing to do with returning the data to render your particular UI panel. Furthermore your API endpoint will tend to get more complex, if the number of API consumers grows. Each consumer may need a different set of fields or related entities from the database. Your API endpoint will stay &lt;a href='https://www.infoq.com/presentations/Simple-Made-Easy/'&gt;simpler&lt;/a&gt;, if you only use it for one particular UI panel of your client-side web application:&lt;/p&gt;&lt;p&gt;&lt;img src="/img/simple-vs-complex-api.svg" alt="Simple vs complex" /&gt;&lt;/p&gt;&lt;p&gt;Especially the 'n + 1' problem leads to myriads of challenges. Like in the example mentioned above, where you might like to render a list of customers and the date of their last order, but the latter is not part of the '/customers' API endpoint. Therefore you need to do n more requests for each customer to the '/orders' endpoint to receive this date. This introduces quite a lot of latency to your singe-page application, since instead of 1 request you need to make n+1 requests.&lt;/p&gt;&lt;p&gt;Technologies like &lt;a href='https://graphql.org/'&gt;GraphQL&lt;/a&gt; and &lt;a href='https://netflix.github.io/falcor/'&gt;Falcor&lt;/a&gt; were designed to avoid this 'n + 1' problem, they allow to fetch all the required data with a single HTTP request. Furthermore the client developer can choose what fields of the requested resources (like '/customers') should be returned.&lt;/p&gt;&lt;p&gt;So why not just use GraphQL. As the name already implies GraphQL was designed as a query language. Similar like backend developers can query the database (with SQL for example), frontend developers can use the GraphQL API to do queries. It was created by an organization (Facebook), where it is common that teams are divided by backend and frontend developers. A GraphQL API serves many different frontend teams or rather their client applications (web, mobile apps etc.). Therefore all the challenges described in the list above need to be addressed. Also the challenges of a public API arise, since even if Facebook didn't offer a public GraphQL API for 3rd-party developers, the sheer amount of frontend developer teams in their organization, would require similar practices.&lt;/p&gt;&lt;p&gt;Don't get me wrong, GraphQL is a good solution for an organization like Facebook. The question is, if it is the right technology for a team of 1-5 developers in a small company. Normally I don't like to quote so-called "laws" like the one of &lt;a href='https://en.wikipedia.org/wiki/Conway%27s_law'&gt;Conway&lt;/a&gt;, but here it describes the situation very well:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations. &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;So GraphQL is a good fit for Facebook, since the separation of frontend and backend developers are baked into their organization and their communication structures.&lt;/p&gt;&lt;p&gt;We are a small team of 4 developers, we have no other types of employees at the moment, so we also do all the business, marketing, product design, customer support etc. But more important all of us do frontend and backend development. We do not need to file a Jira ticket into the backlog of a backend team to get an additional field into a REST-API endpoint (like '/customers'). We can just do this on our own. All of our source code lives in a single Git repository. Therefore the frontend and backend source code for a new feature is part of the same pull request. Even our build process always releases a new version of our frontend and backend at the same time. A technology like GraphQL probably does not provide the optimal trade-offs for a small organization like ours, if it was designed with organizations in mind which are magnitudes larger.&lt;/p&gt;&lt;p&gt;The next blog post will finally get a little bit more technical and show how to implement such a single-page web application that API works more like a classic web application.&lt;/p&gt;</description><pubDate>Sat, 15 Jun 2019 00:00:00 +0200</pubDate></item><item><guid>https://maxweber.github.io/blog/2019-06-08-approaching-the-web-after-tomorrow-part-2</guid><link>https://maxweber.github.io/blog/2019-06-08-approaching-the-web-after-tomorrow-part-2</link><title>Approaching The Web After Tomorrow - Part 2</title><description>&lt;p&gt;This blog post series is about my attempt to implement &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;The Web After Tomorrow&lt;/a&gt; that &lt;a href='https://twitter.com/nikitonsky'&gt;Nikita&lt;/a&gt; described in his &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;blog post&lt;/a&gt; back in 2015.&lt;/p&gt;&lt;p&gt;This part is about some ideas I had to solve the performance challenge that was described in &lt;a href='https://maxweber.github.io/blog/2019-06-04-approaching-the-web-after-tomorrow'&gt;part 1&lt;/a&gt;.&lt;/p&gt;&lt;h2 id="load&amp;#95;only&amp;#95;relevant&amp;#95;data"&gt;Load only relevant data&lt;/h2&gt;&lt;p&gt;Maybe the quickest fix would have been to try to only load datoms which are relevant for the current UI state. This idea is also part of Nikita's proposed solution:&lt;/p&gt;&lt;p&gt;&lt;img src="/img/the-web-after-tomorrow_filters.png" alt="Filters" /&gt;&lt;/p&gt;&lt;p&gt;(source: http://tonsky.me/blog/the-web-after-tomorrow/)&lt;/p&gt;&lt;p&gt;While this is certainly doable, it increases the accidentally complexity, at least in our case. Our DataScript queries had access to the full database portion of the user, this is the layer "Accessible data" in the illustration above. Therefore nothing needed to be fetched from the "Whole database" in advance to ensure that a query will "see" everything it needs to yield the correct result.&lt;/p&gt;&lt;p&gt;However our performance challenge was that the amount of "Accessible data" for a lot of our users had grown too big. Therefore we need to make a change to only load the relevant data for the current UI state, which is the "Looking at data" in illustration above.&lt;/p&gt;&lt;p&gt;The "Looking at data" is only a subset of the "Accessible data" (which is a subset of the "Whole database"). You could also say that the "Looking at data" is derived from the "Accessible data" by applying a filter function for example. Another way would be to execute a database query (SQL, datalog etc.) to the "Accessible data" that yields the "Looking at data". In the context of a database this is often called a view. These views need to be kept up to date as soon as relevant data is transacted into the database. This tends to cause performance challenges. That's why there are a lot of optimizations available for this topic (like materialized views for example).&lt;/p&gt;&lt;p&gt;I do not want to dive deeper into this topic here. I only like to mention a few interesting approaches:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href='https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/'&gt;Turning the database inside-out&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='https://www.youtube.com/watch?v=ZgqFlowyfTA'&gt;Reactive Datalog for Datomic&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='https://www.clockworks.io/2018/09/13/incremental-datalog.html'&gt;Incremental Datalog with Differential Dataflows&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While all these topics are super interesting to me, they have one thing in common:&lt;/p&gt;&lt;p&gt;They are probably not the core of your business, except your company is offering a database product or service. Thereby you end up spending a lot of effort building a half-backed database, instead of focusing on improving the app or service that your company is offering.&lt;/p&gt;&lt;h2 id="a&amp;#95;rest&amp;#95;api"&gt;A REST API&lt;/h2&gt;&lt;p&gt;Another way would have been to switch back to a "classic" REST API. This would have mean that we need to write custom logic to mimic the behaviour of our current real-time web app. A few examples:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;An user can upload new images or videos to Storrito and the new  entries automatically appear in his gallery UI.&lt;/li&gt;&lt;li&gt;Also his team members will see those new entries in their gallery UI  instantly.&lt;/li&gt;&lt;li&gt;Our servers convert an uploaded video into the right format,  meanwhile the UI displays a hint that the video is currently  converted.&lt;/li&gt;&lt;li&gt;The Storrito web app informs the user with a notification, when the  server posts his story and decreases the amount of remaining story  posts (before the user has to buy new ones).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As developers we have grown to like the automatic real-time behaviour of our web app. We just need to transact a change to Datomic and the UI of the affected user(s) will reflect this change.&lt;/p&gt;&lt;p&gt;To notify the user that we have posted his story, the server transacts the date of execution to the database entity that represents this scheduled story post. The new datoms are pushed to the browser of the user and are added to his local Datascript database. This triggers a re-execution of the corresponding datalog queries and this causes that the affected React UI elements are re-rendered. The user then sees the corresponding notification UI about the successful story post. Also other parts of the UI like the amount of remaining story posts updates automatically. In non real-time web apps this is often inconsistent, most of us have probably seen some messenger app, where the unread message icon does not disappear, after you have opened the new message(s). Only a page reload makes the unread message icon go away.&lt;/p&gt;&lt;h2 id="your&amp;#95;own&amp;#95;database&amp;#95;index"&gt;Your own database index&lt;/h2&gt;&lt;p&gt;Instead of loading the datoms, another interesting idea is to provide the front-end with access to the database index. It would only cover the datoms, which are owned by the user (the "Accessible data"). Datomic's database index is implemented with a data structure that is similar to the persistent immutable data structures of Clojure. Both are tree data structures, but the one of Datomic must have way more entries per node to compensate the IO latency, similar to a B-Tree index of a relational database. Due to the immutable nature of this data structure it could be cached without any complex invalidations. This caching could also be done in the browser, whereby it would become something similar like a Datomic peer. Regrettably, Datomic's closed-source model makes this direction impractical.&lt;/p&gt;&lt;p&gt;An alternative is the so-called &lt;a href='https://github.com/datacrypt-project/hitchhiker-tree'&gt;Hitchhiker Tree&lt;/a&gt;, which is open-source and offers similar characteristics like the Datomic index data structure. Transit JSON could be used as storage format, so that the browser can read it directly (Datomic is using the &lt;a href='https://github.com/Datomic/fressian'&gt;Fressian&lt;/a&gt; format).&lt;/p&gt;&lt;p&gt;However in the end you would run into the same issue like described above that you implement a half-baked database, which is a huge distraction from the core of your business. Another limitation is that the XHR request in the browser is asynchronous (the synchronous version is deprecated). To be able to fetch more index segments via XHR requests the DataScript API would need to become asynchronous too. And this would also change the developer experience a lot, since the entity API would not be practical anymore.&lt;/p&gt;&lt;h2 id="graphql"&gt;GraphQL&lt;/h2&gt;&lt;p&gt;There is no doubt that &lt;a href='https://graphql.org/'&gt;GraphQL&lt;/a&gt; is in vogue nowadays and it has been adapted by many organizations already. Despite the good ecosystem (there is even a &lt;a href='https://github.com/walmartlabs/lacinia'&gt;Clojure GraphQL lib&lt;/a&gt;) and tools which are available for GraphQL, it still means a lot of effort to implement a GraphQL back-end. Especially for a small developer team (like ours) this can become a huge burden. Therefore I thought about, if we would pick the optimal &lt;a href='https://www.reddit.com/r/programming/comments/3va6x0/programmers_know_the_benefits_of_everything_and/'&gt;trade-offs&lt;/a&gt; for our small company, if we would adopt a technology like GraphQL (or a similar one like the &lt;a href='http://edn-query-language.org/'&gt;EDN Query Language&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;For that reason the next part of this blog post series will be about reconsidering some common trade-offs of modern web development. Picking different trade-offs can yield huge productivity gains for small teams of full stack developers, which do not have a split between front-end and back-end developer teams.&lt;/p&gt;</description><pubDate>Sat, 08 Jun 2019 00:00:00 +0200</pubDate></item><item><guid>https://maxweber.github.io/blog/2019-06-04-approaching-the-web-after-tomorrow</guid><link>https://maxweber.github.io/blog/2019-06-04-approaching-the-web-after-tomorrow</link><title>Approaching The Web After Tomorrow - Part 1</title><description>&lt;p&gt;My current venture is &lt;a href='https://storrito.com/'&gt;Storrito.com&lt;/a&gt; where we had the chance to develop a system with Clojure, ClojureScript and Datomic from the ground up.&lt;/p&gt;&lt;p&gt;This blog post series is about my attempt to implement &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;The Web After Tomorrow&lt;/a&gt; that &lt;a href='https://twitter.com/nikitonsky'&gt;Nikita&lt;/a&gt; described in his &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;blog post&lt;/a&gt; back in 2015.&lt;/p&gt;&lt;p&gt;In this blog post Nikita describes how modern real-time web applications should work. While the technology exists many web applications are still not fully real-time. Instead there are often multiple sections which have different levels of staleness. As an example he shows the Facebook web application:&lt;/p&gt;&lt;p&gt;&lt;img src="/img/the-web-after-tomorrow_facebook-example.png" alt="Facebook example" /&gt; (image source: https://tonsky.me/blog/the-web-after-tomorrow/)&lt;/p&gt;&lt;p&gt;There are sections like the sidebar menu which are never refreshed after the page load. While other sections like the messenger and the unread message count receive real-time updates.&lt;/p&gt;&lt;p&gt;At the end of the blog post Nikita describes how a full real-time web application might be implemented by using a combination of Datomic and DataScript. For the initial implementation of Storrito I followed this concept idea and tried to build a full real-time web application. On the server we were already using Datomic. On the client-side we added DataScript to our Reagent-based ClojureScript app.&lt;/p&gt;&lt;p&gt;To avoid the complexities of premature performance optimizations and to have a first working version ready, I took an extreme shortcut. Instead of only loading the datoms which are relevant for the current UI state, we just loaded the complete customer database on the initial page load.&lt;/p&gt;&lt;p&gt;This shortcut worked much longer than expected, since Storrito is a single-page application (SPA) and our users only needed to wait a little bit longer at the initial page load. Afterwards all changes were pushed in the form of small deltas (datoms) to the client.&lt;/p&gt;&lt;p&gt;The database portion that needed to be loaded was very small for a normal user (only a few hundred datoms). But the number of datoms grows every time the user creates or changes entities. Most of the users had to wait between 1-3 seconds at the initial page load.&lt;/p&gt;&lt;p&gt;Luckily from a business perspective there were and still are many very frequent users, which creates hundreds or even thousands of new datoms every day. They needed to wait around 20-40 seconds for the initial page load. As you can imaging loading the entire customer database had become an unacceptable option soon.&lt;/p&gt;&lt;p&gt;In the next blog post I will describe some of the ideas we had how to solve this challenge.&lt;/p&gt;</description><pubDate>Tue, 04 Jun 2019 00:00:00 +0200</pubDate></item></channel></rss>