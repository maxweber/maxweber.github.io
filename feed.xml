<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><atom:link href="https://maxweber.github.io/" rel="self" type="application/rss+xml"/><title>The blog of Max Weber</title><link>https://maxweber.github.io/</link><description>A blog about software and business development</description><lastBuildDate>Sun, 09 Jun 2019 11:32:06 +0200</lastBuildDate><generator>clj-rss</generator><item><guid>https://maxweber.github.io/blog/2019-06-08-approaching-the-web-after-tomorrow-part-2</guid><link>https://maxweber.github.io/blog/2019-06-08-approaching-the-web-after-tomorrow-part-2</link><title>Approaching The Web After Tomorrow - Part 2</title><description>&lt;p&gt;This blog post series is about my attempt to implement &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;The Web After Tomorrow&lt;/a&gt; that &lt;a href='https://twitter.com/nikitonsky'&gt;Nikita&lt;/a&gt; described in his &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;blog post&lt;/a&gt; back in 2015.&lt;/p&gt;&lt;p&gt;This part is about some ideas I had to solve the performance challenge that was described in &lt;a href='https://maxweber.github.io/blog/2019-06-04-approaching-the-web-after-tomorrow'&gt;part 1&lt;/a&gt;.&lt;/p&gt;&lt;h2 id="load&amp;#95;only&amp;#95;relevant&amp;#95;data"&gt;Load only relevant data&lt;/h2&gt;&lt;p&gt;Maybe the quickest fix would have been to try to only load datoms which are relevant for the current UI state. This idea is also part of Nikita's proposed solution:&lt;/p&gt;&lt;p&gt;&lt;img src="http://tonsky.me/blog/the-web-after-tomorrow/filters.png" alt="Filters" /&gt;&lt;/p&gt;&lt;p&gt;(source: http://tonsky.me/blog/the-web-after-tomorrow/)&lt;/p&gt;&lt;p&gt;While this is certainly doable, it would increase the accidentally complexity, at least in our case. Our DataScript queries had access to the full database portion of the user, this is the layer "Accessible data" in the illustration above. Therefore nothing needed to be fetched from the "Whole database" in advance to ensure that a query will "see" everything it needs to yield the correct result.&lt;/p&gt;&lt;p&gt;However our performance challenge was that the amount of "Accessible data" for a lot of our users had grown to big. Therefore we need to make a change to only load the relevant data for the current UI state, which is the "Looking at data" in illustration above.&lt;/p&gt;&lt;p&gt;The "Looking at data" is only a subset of the "Accessible data" (which is a subset of the "Whole database"). You could also say that the "Looking at data" is derived from the "Accessible data" by applying a filter function for example. Another way would be to execute a database query (SQL, datalog etc.) to the "Accessible data" that yields the "Looking at data". In the context of a database this is often called a view. These views need to be kept up to date as soon as relevant data is transacted into the database. This tends to cause performance challenges. That's why there are a lot of optimizations available for this topic (like materialized views for example).&lt;/p&gt;&lt;p&gt;I do not want to dive deeper into this topic here. I only like to mention a few interesting approaches:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href='https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/'&gt;Turning the database inside-out&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='https://www.youtube.com/watch?v=ZgqFlowyfTA'&gt;Reactive Datalog for Datomic&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href='https://www.clockworks.io/2018/09/13/incremental-datalog.html'&gt;Incremental Datalog with Differential Dataflows&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While all these topics are super interesting to me, they have one thing in common:&lt;/p&gt;&lt;p&gt;They are probably not the core of your business, except your company is offering a database product or service. Thereby you end up spending a lot of effort building a half-backed database, instead of focusing on improving the app or service that your company is offering.&lt;/p&gt;&lt;h2 id="a&amp;#95;rest&amp;#95;api"&gt;A REST API&lt;/h2&gt;&lt;p&gt;Another way would have been to switch back to a "classic" REST API. This would have mean that we need to write custom logic to mimic the behaviour of our current real-time web app. A few examples:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;An user can upload new images or videos to Storrito and the new  entries automatically appear in his gallery UI.&lt;/li&gt;&lt;li&gt;Also his team members will see those new entries in their gallery UI  instantly.&lt;/li&gt;&lt;li&gt;Our servers convert an uploaded video into the right format,  meanwhile the UI displays a hint that the video is currently  converted.&lt;/li&gt;&lt;li&gt;The Storrito web app informs the user with a notification, when the  server posts his story and decreases the amount of remaining story  posts (before the user has to buy new ones).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As developers we have grown to like the automatic real-time behaviour of our web app. We just need to transact a change to Datomic and the UI of the affected user(s) will reflect this change.&lt;/p&gt;&lt;p&gt;To notify the user that we have posted his story, the server transacts the date of execution to the database entity that represents this scheduled story post. The new datoms are pushed to the browser of the user and are added to his local Datascript database. This triggers a re-execution of the corresponding datalog queries and this causes that the affected React UI elements are re-rendered. The user then sees the corresponding notification UI about the successful story post. Also other parts of the UI like the amount of remaining story posts updates automatically. In non real-time web apps this is often inconsistent, most of us have probably seen some messenger app, where the unread message icon does not disappear, after you have opened the new message(s). Only a page reload makes the unread message icon go away.&lt;/p&gt;&lt;h2 id="your&amp;#95;own&amp;#95;database&amp;#95;index"&gt;Your own database index&lt;/h2&gt;&lt;p&gt;Instead of loading the datoms, another interesting idea is to provide the front-end with access to the database index. It would only cover the datoms, which are owned by the user (the "Accessible data"). Datomic's database index is implemented with a data structure that is similar to the persistent immutable data structures of Clojure. Both are tree data structures, but the one of Datomic must have way more entries per node to compensate the IO latency, similar to a B-Tree index of a relational database. Due to the immutable nature of this data structure it could be cached without any complex invalidations. This caching could also be done in the browser, whereby it would become something similar like a Datomic peer. Regrettably, Datomic's closed-source model makes this direction impractical.&lt;/p&gt;&lt;p&gt;An alternative is the so-called [Hitchhiker Tree](https://github.com/datacrypt-project/hitchhiker-tree), which is open-source and offers similar characteristics like the Datomic index data structure. Transit JSON could be used as storage format, so that the browser can read it directly (Datomic is using the &lt;a href='https://github.com/Datomic/fressian'&gt;Fressian&lt;/a&gt; format).&lt;/p&gt;&lt;p&gt;However in the end you would run into the same issue like described above that you implement a half-baked database, which is a huge distraction from the core of your business. Another limitation is that the XHR request in the browser is asynchronous (the synchronous version is deprecated). To be able to fetch more index segments via XHR requests the DataScript API would need to become asynchronous, too. And this would also change the developer experience a lot, since the entity API would not be practical anymore.&lt;/p&gt;&lt;h2 id="graphql"&gt;GraphQL&lt;/h2&gt;&lt;p&gt;There is no doubt that &lt;a href='https://graphql.org/'&gt;GraphQL&lt;/a&gt; is in vogue nowadays and it has been adapted by many organizations already. Despite the good ecosystem (there is even a &lt;a href='https://github.com/walmartlabs/lacinia'&gt;Clojure GraphQL lib&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;and tools which are available for GraphQL, it still means a lot of effort to implement a GraphQL back-end. Especially for a small developer team (like ours) this can become a huge burden. Therefore I thought about, if we would pick the optimal &lt;a href='https://www.reddit.com/r/programming/comments/3va6x0/programmers_know_the_benefits_of_everything_and/'&gt;trade-offs&lt;/a&gt; for our small company, if we would adapt a technology like GraphQL (see also &lt;a href='http://edn-query-language.org/'&gt;EDN Query Language&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;For that reason the next part of this blog post series will be about reconsidering some common trade-offs of modern web development. Picking different trade-offs can yield huge productivity gains for small teams of full stack developers, which do not have a split between front-end and back-end developer teams.&lt;/p&gt;</description><pubDate>Sat, 08 Jun 2019 00:00:00 +0200</pubDate></item><item><guid>https://maxweber.github.io/blog/2019-06-04-approaching-the-web-after-tomorrow</guid><link>https://maxweber.github.io/blog/2019-06-04-approaching-the-web-after-tomorrow</link><title>Approaching The Web After Tomorrow - Part 1</title><description>&lt;p&gt;My current venture is &lt;a href='https://storrito.com/'&gt;Storrito.com&lt;/a&gt; where we had the chance to develop a system with Clojure, ClojureScript and Datomic from the ground up.&lt;/p&gt;&lt;p&gt;This blog post series is about my attempt to implement &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;The Web After Tomorrow&lt;/a&gt; that &lt;a href='https://twitter.com/nikitonsky'&gt;Nikita&lt;/a&gt; described in his &lt;a href='https://tonsky.me/blog/the-web-after-tomorrow/'&gt;blog post&lt;/a&gt; back in 2015.&lt;/p&gt;&lt;p&gt;In this blog post Nikita describes how modern real-time web applications should work. While the technology exists many web applications are still not fully real-time. Instead there are often multiple sections which have different levels of staleness. As an example he shows the Facebook web application:&lt;/p&gt;&lt;p&gt;&lt;img src="/img/the-web-after-tomorrow_facebook-example.png" alt="Facebook example" /&gt; (image source: https://tonsky.me/blog/the-web-after-tomorrow/)&lt;/p&gt;&lt;p&gt;There are sections like the sidebar menu which are never refreshed after the page load. While other sections like the messenger and the unread message count receive real-time updates.&lt;/p&gt;&lt;p&gt;At the end of the blog post Nikita describes how a full real-time web application might be implemented by using a combination of Datomic and DataScript. For the initial implementation of Storrito I followed this concept idea and tried to build a full real-time web application. On the server we were already using Datomic. On the client-side we added DataScript to our Reagent-based ClojureScript app.&lt;/p&gt;&lt;p&gt;To avoid the complexities of premature performance optimizations and to have a first working version ready, I took an extreme shortcut. Instead of only loading the datoms which are relevant for the current UI state, we just loaded the complete customer database on the initial page load.&lt;/p&gt;&lt;p&gt;This shortcut worked much longer than expected, since Storrito is a single-page application (SPA) and our users only needed to wait a little bit longer at the initial page load. Afterwards all changes were pushed in the form of small deltas (datoms) to the client.&lt;/p&gt;&lt;p&gt;The database portion that needed to be loaded was very small for a normal user (only a few hundred datoms). But the number of datoms grows every time the user creates or changes entities. Most of the users had to wait between 1-3 seconds at the initial page load.&lt;/p&gt;&lt;p&gt;Luckily from a business perspective there were and still are many very frequent users, which creates hundreds or even thousands of new datoms every day. They needed to wait around 20-40 seconds for the initial page load. As you can imaging loading the entire customer database had become an unacceptable option soon.&lt;/p&gt;&lt;p&gt;In the next blog post I will describe some of the ideas we had how to solve this challenge.&lt;/p&gt;</description><pubDate>Tue, 04 Jun 2019 00:00:00 +0200</pubDate></item></channel></rss>